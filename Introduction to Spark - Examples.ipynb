{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark\n",
    "\n",
    "* In memory distributed computing engine\n",
    "* Faster than Hadoop (upto 100x)\n",
    "* Less coding effort (5-10x)\n",
    "* Interactive or batch processing\n",
    "* Built-in rich set of functionalities\n",
    "\n",
    "<img src=\"./images/spark.png\" width=\"400\" height=\"200\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cluster.png\" width=\"800\" height=\"400\" />\n",
    "\n",
    "Image source: https://spark.apache.org/docs/1.1.1/img/cluster-overview.png\n",
    "\n",
    "**Spark Context**: It holds a connection with Spark cluster manager and acts as a client. It is also the coordinator of all spark processes running for the application. \n",
    "\n",
    "**Driver**: A driver is incharge of the process of running the main() function of an application and creating the SparkContext. \n",
    "\n",
    "**A worker**: A worker is any node that can run program in the cluster. \n",
    "\n",
    "**Cluster Manager**: Cluster manager allocates resources to each application in driver program. There are three types of cluster managers supported by Apache Spark â€“ Standalone, Mesos and YARN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python vs Scala:\n",
    "\n",
    "Spark computation in Python is much slower than in Scala.\n",
    "* Scala is native language for Spark (because Spark itself written in Scala).\n",
    "* Scala is a compiled language where as Python is an interpreted language.\n",
    "* Python has process based executors where as Scala has thread based executors.\n",
    "* Python is not a JVM (java virtual machine) language.\n",
    " \n",
    "Many people ask whether it is really necessary to learn Scala to use Spark. Here's an answer. \n",
    "* If you plan to process serious data across nodes in a large cluster, choose Scala.\n",
    "* However, for most users, Python is sufficient.\n",
    "\n",
    "\n",
    "#### Apache Spark data representations: RDD and Dataframe\n",
    "\n",
    "* **RDD** (Resilient Distributed Database) is a collection of immutable distributed elements of your data, partitioned across nodes in a spark cluster. \n",
    "\n",
    "* **Dataframe**, like an RDD, is a collection of immutable distributed data. Unlike an RDD, data is organized into named columns, like a table in a relational database. \n",
    "\n",
    "* **DataSet** has recently been introduced (will not be covered in the class).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD and map, filter, reduce, etc.... \n",
    "We can apply 2 types of operations on RDDs:\n",
    "\n",
    "**Transformation**: Transformation refers to the operation applied on a RDD to create new RDD(s). <br>\n",
    "**Action**: Actions refer to an operation which also apply on RDD that perform computation and send the result back to driver.\n",
    "\n",
    "Example: Map (Transformation) performs operation on each element of RDD and returns a new RDD. But, in case of Reduce (Action), it reduces / aggregates the output of a map by applying some functions (Reduce by key). There are many transformations and actions are defined in Apache Spark documentation, \n",
    "\n",
    "Transformations are *_lazy_*, i.e. are not executed immediately. Only after calling an action are transformations executed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rdd_transformation.png\" width = 600 height = 300/>\n",
    "<img src=\"./images/spark-rdd-trasf-action.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two common ways to create RDD\n",
    "* **_parallelize_** creates an RDD from a list\n",
    "* **_textFile_** creates an RDD from a text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Transformations \n",
    "\n",
    "* _**map**(func)_\n",
    "* _**flatMap**(func)_\n",
    "* _filter(func)_\n",
    "* _mapPartitions(func)_\n",
    "* _mapPartitionWithIndex()_\n",
    "* _union(dataset)_\n",
    "* _intersection(dataset)_\n",
    "* _distict()_\n",
    "* _groupByKey()_\n",
    "* _**reduceByKey**()_\n",
    "* _**sortByKey**()_\n",
    "* ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "### map vs flatMap and filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelize \n",
    "x = sc.parallelize([1,2,3,4])\n",
    "y = x.map(lambda x: (x, x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (2, 4), (3, 9), (4, 16)]\n"
     ]
    }
   ],
   "source": [
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "print(x.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map vs flatMap\n",
    "f = x.flatMap(lambda x: (x, x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 4, 3, 9, 4, 16]\n"
     ]
    }
   ],
   "source": [
    "print(f.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter\n",
    "z = f.filter(lambda x: x % 2 == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 3, 9]\n"
     ]
    }
   ],
   "source": [
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### distinct, union, and intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 1, 9, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "#distinct\n",
    "print(f.distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = sc.parallelize([1,2,3,4,5])\n",
    "r2 = sc.parallelize([2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "#intersection\n",
    "print(r1.intersection(r2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define string list\n",
    "a = ['hello world', 'what a wonderful world', 'Maintain a simple life']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra = sc.parallelize(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello world', 0), ('what a wonderful world', 1), ('Maintain a simple life', 2)]\n"
     ]
    }
   ],
   "source": [
    "#make a an rdd and zip with index\n",
    "raidx = ra.zipWithIndex()\n",
    "print(raidx.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign indices to individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbidx = ra.flatMap(lambda x: x.split(' ')).distinct().zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('wonderful', 0), ('Maintain', 1), ('hello', 2), ('a', 3), ('life', 4), ('what', 5), ('world', 6), ('simple', 7)]\n"
     ]
    }
   ],
   "source": [
    "print(rbidx.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert it (ziped and indexed) to dictionary\n",
    "mydict = rbidx.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wonderful': 0, 'Maintain': 1, 'hello': 2, 'a': 3, 'life': 4, 'what': 5, 'world': 6, 'simple': 7}\n"
     ]
    }
   ],
   "source": [
    "print(mydict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### operations on (key, value)\n",
    "sortByKey and reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([('a',1), ('b',2), ('c', 3), ('a', 3), ('b', 4), ('a', 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('b', 2), ('c', 3), ('a', 3), ('b', 4), ('a', 10)]\n"
     ]
    }
   ],
   "source": [
    "#print it \n",
    "print(x.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('a', 3), ('a', 10), ('b', 2), ('b', 4), ('c', 3)]\n"
     ]
    }
   ],
   "source": [
    "#sortByKey\n",
    "print(x.sortByKey().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 14), ('b', 6), ('c', 3)]\n"
     ]
    }
   ],
   "source": [
    "#reduceByKey\n",
    "print(x.reduceByKey(lambda x,y: x+y).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrow vs Wide Transformations\n",
    "\n",
    "Recall a transformation is applied on an **rdd** and creates another (or possibly many) **rdd**(s).\n",
    "\n",
    "\n",
    "A transformation can be **_narrow_** or **_wide_** depending on whether shuffling of data across partitions is required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rdd.png\" width=\"800\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Actions\n",
    "\n",
    "* _count()_\n",
    "* _**collect**()_\n",
    "* _**take**(n)_\n",
    "* _top(n)_\n",
    "* _**reduce**()_\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Computation of Pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/mcpi.png\" width=\"800\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.141856\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "NUM_SAMPLES = 5000000\n",
    "def inside(a):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, NUM_SAMPLES)).filter(inside).count()\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q) What does **_sc.parallelize(range(0, NUM_SAMPLES)).filter(inside)_** return?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load text data into RDD\n",
    "\n",
    "Consider the following sample Emails\n",
    "<pre>\n",
    "...\n",
    "Qaddafi's cousin, Col. Ali Qaddafiddam had failed in efforts to recruit fighters among the\n",
    "Egyptian population living immediately across the border with Libya.\n",
    "These individuals added that during the week of February 21 the Libyan Leader spoke to Syrian\n",
    "President Bashir al-Assad on at least three occasions by secure telephone lines. During the\n",
    "conversations Qaddafi asked that Syrian officers and technicians currently training the Libyan\n",
    "Air Force be placed under command of the Libyan Army and allowed to fight against the rebel\n",
    "forces.\n",
    "(Source Comment: Senior Libyan Army officers still loyal to Qaddafi added that On February\n",
    "23, President Assad told General Isam Hallaq, the commander in chief of the Syrian Air Force,\n",
    "to instruct the pilots and technicians in Tripoli to help the Libyan regime, should full scale Civil\n",
    "War breaks out in the immediate future.)\n",
    "On March 2, a military officer with ties to Qaddfi's son Khamis stated privately that the number\n",
    "of Libyan pilots defecting to the opposition has destroyed the morale and professional spirit of\n",
    "the Libyan Air Force at this critical moment, when Tripoli's air superiority is its principal weapon\n",
    "against insurgents. In the opinion of this individual Qaddafi and his senior military advisors are\n",
    "convinced that the European Union and the U.S will impose a no-fly zone over Libya in the\n",
    "immediate future. These advisors believe that the no fly zone will serve as air support for\n",
    "opposition forces. They are also prepared for the Western allies to bomb anti-aircraft facilities in\n",
    "and around Tripoli in preparation for the establishment of the no-fly zone. Foreign Minister\n",
    "Mousa Kousa is convinced that that Russia and Turkey will oppose the move, and may prevent\n",
    "the implementation of the no fly zone.\n",
    "...\n",
    "</pre>\n",
    "\n",
    "### Steps to perform word counts\n",
    "1. read data as an RDD of lines\n",
    "2. filter out empty lines\n",
    "3. split each line into words\n",
    "4. convert each word into (key/value) pair\n",
    "5. reduce them by key \n",
    "6. flip k/v to v/k for sorting\n",
    "7. sort by key in descending order\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/wordcount.png\" width=\"1000\" height=\"800\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us first load data into RDD\n",
    "lines = sc.textFile('data/sample.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1,C05739545,WOW,H,\"Sullivan, Jacob J\",87,2012-09-12T04:00:00+00:00,2015-05-22T04:00:00+00:00,DOCUMENTS/HRC_Email_1_296/HRCH2/DOC_0C05739545/C05739545.pdf,F-2015-04841,HRC_Email_296,FW: Wow,,\"Sullivan, Jacob J <Sullivan11@state.gov>\",,\"Wednesday, September 12, 2012 10:16 AM\",F-2015-04841,C05739545,05/13/2015,RELEASE IN FULL,,\"UNCLASSIFIED']\n"
     ]
    }
   ],
   "source": [
    "print(lines.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create non_empty lines\n",
    "non_empty = lines.filter(lambda x: len(x) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up lines by replacing characters and numbers, etc.\n",
    "import re\n",
    "new_lines = non_empty.map(lambda x: re.sub('[,/_\\.]', '',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1C05739545WOWH\"Sullivan Jacob J\"872012-09-12T04:00:00+00:002015-05-22T04:00:00+00:00DOCUMENTSHRCEmail1296HRCH2DOC0C05739545C05739545.pdfF-2015-04841HRCEmail296FW: Wow\"Sullivan Jacob J <Sullivan11@state.gov>\"\"Wednesday September 12 2012 10:16 AM\"F-2015-04841C0573954505132015RELEASE IN FULL\"UNCLASSIFIED']\n"
     ]
    }
   ],
   "source": [
    "print(new_lines.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider only words of two characters or more \n",
    "words = new_lines.flatMap(lambda x: x.split(' ')).filter(lambda x: len(x) > 1).map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['the', 'to', 'of', 'in', 'and', 'that']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "finals = words.filter(lambda x: x not in stop_words).map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1c05739545wowh\"sullivan', 'jacob', 'j\"872012-09-12t04:00:00+00:002015-05-22t04:00:00+00:00documentshrcemail1296hrch2doc0c05739545c05739545.pdff-2015-04841hrcemail296fw:', 'wow\"sullivan', 'jacob', '<sullivan11@state.gov>\"\"wednesday', 'september', '12', '2012', '10:16']\n"
     ]
    }
   ],
   "source": [
    "print(words.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('no', 27),\n",
       " ('is', 18),\n",
       " ('on', 18),\n",
       " ('qaddafi', 18),\n",
       " ('state', 17),\n",
       " ('libyan', 11),\n",
       " ('benghazi', 10),\n",
       " ('with', 10),\n",
       " ('us', 9),\n",
       " ('agreement', 9)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduceByKey and show the 10 most frequent words\n",
    "\n",
    "\n",
    "finals.reduceByKey(lambda x,y: x+y).top(10, lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "stop_words = ['the', 'to', 'of', 'in']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducyByKey again and show the 10 most frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we need to further remove non-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DataFrame-in-Spark.png\" width=\"600\" height=\"400\" /> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
