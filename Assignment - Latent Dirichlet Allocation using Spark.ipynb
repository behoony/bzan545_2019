{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling with Latent Dirichlet Allocation\n",
    "\n",
    "\"_In machine learning and natural language processing, a topic model is a type of statistical <br>\n",
    "model for discovering the abstract “topics” that occur in a collection of documents. (from Wikipedia)_\"\n",
    "\n",
    "A group of words (i.e topic) from a collection of documents that best represents the information in the collection.\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) represents documents as mixtures of topics that spit out words with certain probabilities. <br>\n",
    "<font color=blue>More precisely, a topic is a probability distribution over the entire vocabulary.</font>\n",
    "\n",
    "<img src=\"./images/lda.png\"  width=\"800\" height=\"400\" />\n",
    "<img src=\"./images/lda_graphical.png\" width=\"800\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Procedure Using PySpark\n",
    "<img src=\"./images/overall.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Description of the Assignment\n",
    "\n",
    "You are to apply LDA to Mr. Trump's tweet data and infer topics that represent the entire tweet data. From topic modeling perspective, a tweet is considered a document, and the whole collection of tweets corpus. For the assignment, you need to process the original input and produce a sparse vector representation which will be passed into Spark's LDA package.\n",
    "\n",
    "Data is located at: _data/trump.csv_ and its format is described in _data/trump_header.txt_\n",
    "\n",
    "Show your steps below and submit your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm up excercise\n",
    "\n",
    "#### Compute the total number of tweets per year, and per month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Setting up SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "sc = SparkContext(conf=SparkConf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict,OrderedDict\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create an RDD of Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Filter out unneccesary characters, numbers, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Remove stop words. Each RDD element is now a bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create a dictionary of unique words in the corpus. Also create an inverse dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Convert each bag of words into a sparse vector with ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Apply LDA, create a model, and print out topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint for Step 5.\n",
    "dict_of_doc = { 4: 3, 2: 4, 5: 5}\n",
    "sorted_keys, sorted_values = zip(*sorted(dict_of_doc.items()))\n",
    "print(sorted_keys, sorted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint for Step 5.\n",
    "def to_sparseVector(doc): \n",
    "    # create dict_of_doc, sorted_keys, sorted_values here...\n",
    "    return [id, Vectors.sparse(len(dict_of_doc), \n",
    "                               sorted_keys, \n",
    "                               sorted_values)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = raw_rdd.map(to_sparseVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5\n",
    "num_words_in_topic = 10\n",
    "\n",
    "lda_model = LDA.train(corpus, k=num_topics, maxIterations=50)\n",
    "topics = lda_model.describeTopics(maxTermsPerTopic=num_words_in_topic)\n",
    "\n",
    "for idx in range(num_topics):\n",
    "    print(\"Topic #{0}\".format(idx))\n",
    "    for i in range(num_words_in_topic):\n",
    "        print(\"  {0}\\t{1}\".format(inverse_dictionary[topics[idx][0][i]], topics[idx][1][i]))\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "print(\"Vocabulary size = {0}\".format(len(dictionary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note***: Topic results are not much meaningful. Many words should have been removed from the vocabulary. How can you improve it? For example, how can you remove top 200 most frequent words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
